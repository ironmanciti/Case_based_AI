{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EKOTlwcmxmej"
   },
   "source": [
    "## 145. BERT Fine-Tuning Tutorial with HuggingFace PyTorch Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJR6t_gCQe_x"
   },
   "source": [
    "- [Chris McCormick and Nick Ryan](https://mccormickml.com/2019/07/22/BERT-fine-tuning/) tutorial 을 기초로 작성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2kTWp0mKIoCx",
    "outputId": "048a92a4-020b-44cd-98ca-187f9e129dc1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import wget\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ElsnSNUridI"
   },
   "source": [
    "- Hugging Face Library 설치\n",
    "    - !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## Loading CoLA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9ZKxKc04Btk"
   },
   "source": [
    "- 단일 문장 분류를 위해 [CoLA (Corpus of Linguistic Acceptability)](https://nyu-mll.github.io/CoLA/) 데이터 세트를 사용  \n",
    "- 문법적으로 정확하거나 틀린 것으로 표시된 문장 세트   \n",
    "- ```GLUE Benchmark```에 포함 된 테스트 중 하나\n",
    "\n",
    "### Download & Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08pO03Ff1BjI"
   },
   "source": [
    "dataset 은 GitHub 에서 repo 가능: https://nyu-mll.github.io/CoLA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dkRVoub6jno0",
    "outputId": "c12d213e-7f8f-48e3-9699-dea545a65358"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "\n",
    "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "\n",
    "if not os.path.exists('./cola_public_1.1.zip'):\n",
    "    wget.download(url, './cola_public_1.1.zip')\n",
    "\n",
    "if not os.path.exists('./cola_public'):\n",
    "    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQUy9Tat2EF_"
   },
   "source": [
    "## 2.2. Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeyVCXT31EZQ"
   },
   "source": [
    "파일 이름에서 `tokenized` 및 `raw` 버전의 데이터를 모두 사용할 수 있음을 알 수 있습니다.\n",
    "\n",
    "**pre-trained BERT 를 적용하려면 BERT 모델에서 제공하는 tokenizer 를 사용해야하므로 `tokenized` version 의 data 는 사용할 수 없습니다.** 이는, \n",
    "\n",
    "\n",
    "(1) BERT 모델에 특정한 고정 어휘가 있고, \n",
    "\n",
    "\n",
    "(2) BERT tokenizer 가 out-of-vocabulary word 를 처리하는 특정 방법을 가지고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "UIYHKq7Y2v5K",
    "outputId": "fc168d8a-12fd-4537-fe0d-e7ff28f7c934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8551, 4)\n",
      "(527, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None,\n",
    "                             names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "df_test = pd.read_csv(\"./cola_public/raw/in_domain_dev.tsv\", delimiter='\\t', header=None,\n",
    "                             names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- label \n",
    "    - 1 : 문법적으로 correct\n",
    "    - 0 : 문법적으로 incorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8126</th>\n",
       "      <td>ad03</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Jason intended for PRO to learn magic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6437</th>\n",
       "      <td>d_98</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Any pilot on duty today must be flying this pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3187</th>\n",
       "      <td>l-93</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Her stepmother always clad her in black.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>l-93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The stick hit the fence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>r-67</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>He'll bring me one if he sees a hot dog.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "8126            ad03      0           *   \n",
       "6437            d_98      0           *   \n",
       "3187            l-93      0           *   \n",
       "2805            l-93      1         NaN   \n",
       "1573            r-67      0           *   \n",
       "\n",
       "                                               sentence  \n",
       "8126             Jason intended for PRO to learn magic.  \n",
       "6437  Any pilot on duty today must be flying this pl...  \n",
       "3187           Her stepmother always clad her in black.  \n",
       "2805                           The stick hit the fence.  \n",
       "1573           He'll bring me one if he sees a hot dog.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-sPZCzQb5oHT",
    "outputId": "b3832760-76b1-48bf-d564-63e5ea0a2eb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>I have some papers to announce that I've got t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5615</th>\n",
       "      <td>Who do you think that saw Bill?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>We elected my father, who had just turned 60, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "1758  I have some papers to announce that I've got t...      0\n",
       "5615                    Who do you think that saw Bill?      0\n",
       "1138  We elected my father, who had just turned 60, ...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문법적으로 not acceptable\n",
    "df_train[df_train['label'] == 0].sample(3)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "RAQ8zp9s_G2x",
    "outputId": "147e0855-30d4-462b-98a7-e926d1234802"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8127</th>\n",
       "      <td>The boys should could all go</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>Fred hired Sharon to change the oil.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>Carmen obtained a spare part from Diana.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>That John was elected surprised Frank.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5735</th>\n",
       "      <td>He turned off the light.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentence  label\n",
       "8127              The boys should could all go      1\n",
       "3964      Fred hired Sharon to change the oil.      1\n",
       "2750  Carmen obtained a spare part from Diana.      1\n",
       "4906    That John was elected surprised Frank.      1\n",
       "5735                  He turned off the light.      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문법적으로 acceptable\n",
    "df_train[df_train['label'] == 1].sample(5)[['sentence', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "WAk6jcIg_G20",
    "outputId": "90f69c1f-8e6b-4362-b92c-82961b6e14f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19f970ac2c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['label'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SMZ5T5Imhlx"
   },
   "source": [
    "- 훈련 세트의 문장과 레이블을 numpy ndarrays로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GuE5BqICAne2"
   },
   "outputs": [],
   "source": [
    "train_sentences = df_train.sentence.values\n",
    "train_labels = df_train.label.values\n",
    "\n",
    "test_sentences = df_test.sentence.values\n",
    "test_labels = df_test.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ex5O1eV-Pfct"
   },
   "source": [
    "# 3. Tokenization & Input Formatting\n",
    "\n",
    "- 데이터 세트를 BERT가 학습 할 수있는 형식으로 변환\n",
    "    - 토큰으로 분할한 다음 tokenizer vocabulary에서 해당 토큰을 색인에 매핑\n",
    "    - BERT에 포함 된 토크나이저에 의해 수행되어야 함\n",
    "    - bert-base-uncased : BERT 기본 size, 소문자 변환된 vocabulary를 가진 모델\n",
    "    \n",
    "## BERT 의 formatting requirement\n",
    "\n",
    "1. 각 문장의 시작과 끝에 특별한 토큰([CLS]와 [SEP])을 추가 \n",
    "2. 모든 문장을 하나의 일정한 길이로 padding 하거나 truncate  \n",
    "3. \"attetion mask\"를 사용하여 real token 과 padding token 을 명시 적으로 구별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8e956671b68b4bd2a3115908aeb4b9cc",
      "e636274e9d8c4cecb07da4aff2deb5ca",
      "f9d120d0574c497bb69493c929917518",
      "261dee99ae0f43888729246ea00790e4",
      "9136ef5cc11a41439e3991ea0c46f3bf",
      "f9b6a7ba136a4829b14f875c1a2e2ba7",
      "af65fe74899446ef84256b4a45910c6b",
      "4c2fa4a84bee4514999642bc37b6ce01"
     ]
    },
    "colab_type": "code",
    "id": "IOQl4De86mal",
    "outputId": "450769ef-7221-4af7-b152-7349bf49a3b8"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFzmtleW6KmJ"
   },
   "source": [
    "### BERT tokenizer methods \n",
    "\n",
    "- tokenizer.tokenize : token 화\n",
    "- tokenizer.convert_tokens_to_ids : token을 id에 mapping  \n",
    "\n",
    "실제로는 모든 문장을 변환 할 때,`tokenize`와`convert_tokens_to_ids`를 별도로 호출하지 않고`tokenize.encode` 함수를 사용하여 두 단계를 모두 처리\n",
    "- tokenizer.encode \n",
    "    - tokenizer.tokenize + \n",
    "    - special token([CLS]와 [SEP]) 추가 + \n",
    "    - tokenizer.convert_tokens_to_ids +\n",
    "    - 모든 sentence를 same length로 truncate  \n",
    "    \n",
    "    \n",
    "- tokenizer.encode_plus \n",
    "    - tokenizer.encode +\n",
    "    - real token과 [PAD] token을 구분하는 attention mask 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dLIbudgfh6F0",
    "outputId": "22be44a6-b2d8-44f7-8899-498fe1841c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
      "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', train_sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(train_sentences[0], max_length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoding용 helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRDsmEb1_G3Q"
   },
   "outputs": [],
   "source": [
    "def encoding(sentences):\n",
    "    input_ids = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "        input_ids.append(encoded_sent)\n",
    "        \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "QAL5xIzRFMFg",
    "outputId": "4567626d-84d3-4e33-98e4-79778e9f7157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 8551\n",
      "원문장 Our friends won't buy this analysis, let alone the next one we propose.\n",
      "ENCODING 문장 [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n",
      "DECODED 문장 [CLS] our friends won't buy this analysis, let alone the next one we propose. [SEP]\n"
     ]
    }
   ],
   "source": [
    "train_ids = encoding(train_sentences)\n",
    "test_ids =  encoding(test_sentences)\n",
    "\n",
    "print('문장길이', len(train_ids))\n",
    "print('원문장', train_sentences[0])\n",
    "print('ENCODING 문장', train_ids[0])\n",
    "print('DECODED 문장', tokenizer.decode(train_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhwCKszh6ych"
   },
   "source": [
    "### Padding & Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([len(sent) for sent in train_ids] + [len(sent) for sent in test_ids], bins=50)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hp-54FcQ_p3h"
   },
   "source": [
    "MAX_LEN = 32를 선택하고 패딩을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "W--bPXmkGmAj",
    "outputId": "7cb9b72e-187c-400a-8d27-f877ca9205f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "[  101  2256  2814  2180  1005  1056  4965  2023  4106  1010  2292  2894\n",
      "  1996  2279  2028  2057 16599  1012   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[  101  1996 11279  8469  1996  9478  3154  1997  1996  5749  1012   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 32\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "train_sequences = pad_sequences(train_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "test_sequences = pad_sequences(test_ids, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "\n",
    "print(train_sequences[0])\n",
    "print(test_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDs-MYtYH8sL"
   },
   "source": [
    "### Attention Masks 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhGulL1pExCT"
   },
   "source": [
    "BERT vocabulary 는 ID 0 을 사용하지 않으므로 토큰 ID가 0 이면 padding 이고 그렇지 않으면 실제 토큰입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kq0j-1dD_G3d"
   },
   "outputs": [],
   "source": [
    "def masking(sequences):\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in sequences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "egJLDslzJTMa",
    "outputId": "27eb2dbe-b5f6-4225-abb5-4363d92e8983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_masks = masking(train_sequences)\n",
    "test_masks = masking(test_sequences)\n",
    "\n",
    "print(train_masks[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LzSbTqW9_BR"
   },
   "source": [
    "### Converting to PyTorch Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RMfP9syLtJT"
   },
   "outputs": [],
   "source": [
    "train_sequences = torch.tensor(train_sequences, dtype=torch.long)\n",
    "test_sequences  = torch.tensor(test_sequences, dtype=torch.long)\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "train_masks  = torch.tensor(train_masks, dtype=torch.long)\n",
    "test_masks  = torch.tensor(test_masks, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dD9i6Z2pG-sN"
   },
   "source": [
    "- torch DataLoader 클래스를 사용하여 Dataset에 대한 iterator를 만듭니다. \n",
    "\n",
    "- 특정 task 에서 BERT를 fine-tuning 할 때 배치 크기가 16 또는 32 로 하라고 recommend 하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiKARlFgMWCD"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# training set 에 대한 DataLoader 생성\n",
    "train_data = TensorDataset(train_sequences, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "# validation set 에 대한 DataLoader 생성\n",
    "test_data = TensorDataset(test_sequences, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bwa6Rts-02-"
   },
   "source": [
    "# BERT Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3xYQ3iLO08SX"
   },
   "source": [
    "입력 데이터가 올바르게 포맷 되었으므로 BERT를 미세 조정할 때입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6TKgyUzPIQc"
   },
   "source": [
    "## 4.1. BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sjzRT1V0zwm"
   },
   "source": [
    "fine-tuning 을 위해 제공되는 현재의 class list:\n",
    "* BertModel\n",
    "* BertForPreTraining\n",
    "* BertForMaskedLM\n",
    "* BertForNextSentencePrediction\n",
    "* BertForSequenceClassification \n",
    "* BertForTokenClassification\n",
    "* BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXYitPoE-cjH"
   },
   "source": [
    "우리가 사용할 것은 [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)입니다.  \n",
    "\n",
    "- sentence classifier 로 사용할 분류를 위한 단일 linear layer가 추가 된 통상적 BERT 모델  \n",
    "\n",
    "- 사전 훈련된 전체 BERT 모델과 추가된 훈련되지 않은 classification layer 가 특정 task 에 대해 훈련된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eliaCzkoR3M9"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2,   # binary,  >= 3 multi-class task\n",
    "    output_attentions=False,        # model 이 attention weights 를 반환할지 여부\n",
    "    output_hidden_states=False)     # model 이 모든 hidden-state 를 반환할지 여부\n",
    "\n",
    "model.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0Jv6c7-HHDW"
   },
   "source": [
    "참고삼아 아래의 names, dimensions of the weights 출력.\n",
    "\n",
    "1. The embedding layer.\n",
    "2. The first of the twelve transformers.\n",
    "3. The output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "id": "tsyQUS7rTpRZ",
    "outputId": "37d7fcce-38bf-4d36-8a64-daccef614432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRWT-D4U_Pvx"
   },
   "source": [
    "## 4.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8o-VEBobKwHk"
   },
   "source": [
    "- fine-tuning 에 다음 값들이 recommend 됩니다. \n",
    "    - 배치 크기 : 16, 32 (DataLoader 를 만들 때 32 를 선택).\n",
    "    - Learning rate(Adam) : 5e-5, 3e-5, 2e-5 (2e-5 사용)\n",
    "    - 에포크 수 : 2, 3, 4 (4를 사용).\n",
    "\n",
    "- The epsilon parameter `eps = 1e-8` zero divide 방지를 위한 매우 작은 숫자임. (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
    "\n",
    "- AdamW(Adam algorithm with weight decay fix) optimizer 는 다음 참조 `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daJaQ6k9Xokg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4VN7mpNBeC6"
   },
   "source": [
    "- get_linear_schedule_with_warmup  \n",
    "\n",
    "    - 학습률이 워밍업 기간 동안 선형적으로 증가한 이후에 선형적으로 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tuRl1po3ZOsh",
    "outputId": "fb1d02d9-6dcc-411b-afdc-9029d022eb6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(total_steps)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                num_warmup_steps=0,  # Default value in run_glue.py\n",
    "                                num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqfmWwUR_Sox"
   },
   "source": [
    "## 4.3. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KNhRtWPXH9C3"
   },
   "source": [
    "경과시간 formatting 을 위한 helper function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpt6tR83keZD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLXYq0JFOhvd"
   },
   "source": [
    "- model.eval() : 모든 layer 가 eval mode 로 동작. batchnorm, dropout layers 가 training mode 아닌 eval mode 로 동작.\n",
    "- torch.no_grad() : autograd engine 을 deactivate 하여 memory 절약 및 speed 향상."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "outputId": "b7887afa-c593-437f-d593-5c0378871b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trimu\\Miniconda3\\envs\\tf20\\lib\\site-packages\\transformers\\optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    268.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    268.    Elapsed: 0:01:02.\n",
      "  Batch   120  of    268.    Elapsed: 0:01:33.\n",
      "  Batch   160  of    268.    Elapsed: 0:02:04.\n",
      "  Batch   200  of    268.    Elapsed: 0:02:35.\n",
      "  Batch   240  of    268.    Elapsed: 0:03:06.\n",
      "\n",
      "  Average training loss: 0.51\n",
      "  Training epcoh took: 0:03:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.80\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    268.    Elapsed: 0:01:02.\n",
      "  Batch   120  of    268.    Elapsed: 0:01:33.\n",
      "  Batch   160  of    268.    Elapsed: 0:02:04.\n",
      "  Batch   200  of    268.    Elapsed: 0:02:36.\n",
      "  Batch   240  of    268.    Elapsed: 0:03:07.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epcoh took: 0:03:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    268.    Elapsed: 0:01:02.\n",
      "  Batch   120  of    268.    Elapsed: 0:01:33.\n",
      "  Batch   160  of    268.    Elapsed: 0:02:05.\n",
      "  Batch   200  of    268.    Elapsed: 0:02:36.\n",
      "  Batch   240  of    268.    Elapsed: 0:03:07.\n",
      "\n",
      "  Average training loss: 0.23\n",
      "  Training epcoh took: 0:03:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.82\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    268.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    268.    Elapsed: 0:01:02.\n",
      "  Batch   120  of    268.    Elapsed: 0:01:34.\n",
      "  Batch   160  of    268.    Elapsed: 0:02:05.\n",
      "  Batch   200  of    268.    Elapsed: 0:02:36.\n",
      "  Batch   240  of    268.    Elapsed: 0:03:07.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:03:28\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  Validation took: 0:00:04\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs): \n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    st = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - st)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'\n",
    "                        .format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, # sentence 구분 [0,0,0,0,1,1,1,1]\n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple (loss, logits, hidden_states, attentions)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # training data 의 평균 loss 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # learning curve 시각화를 위해 loss value 저장.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - st)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # 각 training epoch 이 끝나면, Test set 에 대한 performance 측정.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "            \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "        label_ids = b_labels.cpu().numpy()\n",
    "        \n",
    "        _, y_pred = torch.max(logits, axis=1)\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = sum(y_pred.cpu().numpy() == label_ids) / len(label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - st)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-G03mmwH3aI"
   },
   "source": [
    "### 전체 batch 에 대한 training loss 를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "68xreA9JAmG5",
    "outputId": "2f29f2df-da46-4719-d1a3-1ad606022d40"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "145_BERT_Fine_Tuning_Sentence_Classification_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "261dee99ae0f43888729246ea00790e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c2fa4a84bee4514999642bc37b6ce01",
      "placeholder": "​",
      "style": "IPY_MODEL_af65fe74899446ef84256b4a45910c6b",
      "value": " 232k/232k [00:00&lt;00:00, 307kB/s]"
     }
    },
    "4c2fa4a84bee4514999642bc37b6ce01": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e956671b68b4bd2a3115908aeb4b9cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9d120d0574c497bb69493c929917518",
       "IPY_MODEL_261dee99ae0f43888729246ea00790e4"
      ],
      "layout": "IPY_MODEL_e636274e9d8c4cecb07da4aff2deb5ca"
     }
    },
    "9136ef5cc11a41439e3991ea0c46f3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "af65fe74899446ef84256b4a45910c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e636274e9d8c4cecb07da4aff2deb5ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9b6a7ba136a4829b14f875c1a2e2ba7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d120d0574c497bb69493c929917518": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9b6a7ba136a4829b14f875c1a2e2ba7",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9136ef5cc11a41439e3991ea0c46f3bf",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
